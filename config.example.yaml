# Claude Code Open Configuration
# Generate this file with: cco config generate

# Server configuration
host: 127.0.0.1           # Host to bind to
port: 6970                # Port to listen on
api_key: your-proxy-key   # Optional: API key to protect the proxy

# Provider configurations
providers:
  # OpenRouter - Access to multiple models from different providers
  - name: openrouter
    api_key: your-openrouter-api-key
    # url: https://openrouter.ai/api/v1/chat/completions  # Optional: URL is set automatically
    model_whitelist:       # Optional: restrict to specific model patterns
      - claude             # Allow any model containing "claude"
      - gpt-4             # Allow any model containing "gpt-4"

  # Local LM Studio instance
  - name: local-lmstudio
    url: "http://localhost:1234/v1/chat/completions"
    api_key: "not-needed"  # Local servers typically don't need auth

  # OpenAI - Direct access to GPT models
  - name: openai
    api_key: your-openai-api-key

  # Anthropic - Direct access to Claude models  
  - name: anthropic
    api_key: your-anthropic-api-key

  # Nvidia - Access to Nemotron models
  - name: nvidia
    api_key: your-nvidia-api-key

  # Google Gemini - Access to Gemini models
  - name: gemini
    api_key: your-gemini-api-key

# Domain mappings - route local domains to existing providers
domain_mappings:
  localhost: openai       # Route localhost requests to OpenAI provider
  127.0.0.1: gemini      # Route 127.0.0.1 requests to Gemini provider  
  0.0.0.0: openrouter    # Route 0.0.0.0 requests to OpenRouter provider

# Router configuration for different use cases
router:
  default: local-lmstudio/qwen/qwen3-coder-30b           # Default to local model
  think: openai/o1-preview                               # For complex reasoning
  background: anthropic/claude-3-haiku-20240307         # For background tasks
  long_context: anthropic/claude-3-5-sonnet-20241022    # For long documents
  web_search: openrouter/perplexity/llama-3.1-sonar-huge-128k-online

# Features:
# - domain_mappings allows routing local server requests to existing provider transformations
# - localhost requests will use OpenAI provider's request/response transformation
# - This enables local model support without needing a separate LocalProvider
# - You can map any domain to any configured or built-in provider
# - Built-in providers: openrouter, openai, anthropic, nvidia, gemini
